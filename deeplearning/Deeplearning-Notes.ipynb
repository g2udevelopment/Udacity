{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with numpy and matrices\n",
    "\n",
    "## Math\n",
    "\n",
    "For any matrix C=A*B we define $C(i,j)=\\sum_{k=1}^{m} A(i,k)B(k,j)$\n",
    "\n",
    "## Numpy\n",
    "\n",
    "Element wise operations in numpy. Example is multiplying with a scalar.\n",
    "\n",
    "Could also be done with matrices of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  7  8  9 10]\n",
      "[ 5 10 15 20 25]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "values = [1,2,3,4,5]\n",
    "np_values = np.array(values) + 5\n",
    "t_values = np.multiply(values, 5)\n",
    "print(np_values)\n",
    "print(t_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the shape of an `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication (Matrix Product) of $n*m,m*p$ results in a matrix of shape $n*p$\n",
    "\n",
    "Matrix multiplications is repetition of vector dot product (rows times columns)\n",
    "Matrix multiplication is **not commutative** $A\\times B\\ne B\\times A$\n",
    "\n",
    "This implies for data, that data on the left side has to be laid out as rows and data on the right side as columns\n",
    "\n",
    "## Matrix multiplication in numpy.\n",
    "\n",
    "Can be element wise with `multiply` and `*` or the matrix product with `np.matmul` in numpy the dot product `dot` is the same\n",
    "for 2 dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 3)\n",
      "[[ 9 12 15]\n",
      " [19 26 33]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "c = np.matmul(a,b)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing in numpy\n",
    "\n",
    "Be carefull transpose only changes the indexing so the underlying data is the same.\n",
    "\n",
    "$A\\times B^\\intercal=(B\\times A^\\intercal)^\\intercal$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "d = b.T\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network learning\n",
    "\n",
    "First the perceptron is discussed. After that we need a more general approach to not only fix lines but other shapes\n",
    "\n",
    "### Log Loss function\n",
    "\n",
    "Error function needs to be continous and differentiable in order for gradient descent to work. The more a data point is from the boundary the higher the error (if it is on the wrong side).\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "To move from discrete values to continous values we use an activation function for instance.\n",
    "\n",
    "sigmoid(x) $S(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "Applied to Wx+b this becomes $\\hat{y}=S(Wx+b)$\n",
    "\n",
    "### Softmax\n",
    "\n",
    "If you want to assign a continous value to a multi-class classfication you can use softmax. This works even is the score is < 0. Softmax is equal two sigmoid when number of classes = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894142 0.73105858]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def softmax(L):\n",
    "    return np.exp(L) / (np.sum(np.exp(L)))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+math.e**-x)\n",
    "\n",
    "print(softmax([0,1]))\n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding.\n",
    "\n",
    "Transform multiple classes into columns with 1 and 0's. No linear dependencies.\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "Pick the model that gives the highest percentage to the correct label. By multiply the change of it beeing the correct label\n",
    "\n",
    "### Products vs Sums\n",
    "\n",
    "When multiplying lots of small numbers < 1 the products becomes very, very small and a single changes has a great impact which is of no good so we need to turn products into sums $log(a) + log(b) = log(a*b)$\n",
    "\n",
    "We will use natural log ln. so with four points a,b,c,d a*b*c*d becomes ln(a)+ln(b)+ln(c)+ln(d), because they are all < 1 they are all negative and it is better to take the negative so -ln(a)-ln(b)-ln(c)-ln(d) , this is the  cross-entropy. Lower numbers means a better model. This can be used as error functions because negative log of a small number is a big number (so a big error) and smaller numbers are more mis classified. Our goal is to minimize cross-entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross - Entropy\n",
    "\n",
    "Formula: $-\\sum_{i=1}^{m} y_iln(p_i) + (1-y_i)ln(1-p_i)$\n",
    "\n",
    "Formula holds when there are 2 class yi{0,1}\n",
    "\n",
    "cross entropy for multiple classes\n",
    "\n",
    "$-\\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij}ln(p_{ij})$\n",
    "\n",
    "We can see from this formula that both are the samen for n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, P):\n",
    "    cross_entropy = 0\n",
    "    for idx, yi in enumerate(Y):\n",
    "        pi = P[idx]\n",
    "        cross_entropy += yi*np.log(pi) + (1-yi)*np.log(1-pi)\n",
    "    return -cross_entropy\n",
    "\n",
    "def cross_entropy_np(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "\n",
    "Error function is average 1/m of the cross entropy\n",
    "\n",
    "## Gradient descent step\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
